{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE and MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_large = pd.read_csv(\"../data/poly_regression_large.csv\")\n",
    "X_large, Y_large = data_large[\"x\"], data_large[\"y\"]\n",
    "X_train_large, X_test_large, Y_train_large, Y_test_large = train_test_split(X_large, Y_large, test_size=0.20, random_state=333, shuffle=True)\n",
    "_, _, Y_train_large, Y_test_large = np.array(X_train_large), np.array(X_test_large), np.array(Y_train_large), np.array(Y_test_large)\n",
    "\n",
    "data_small = pd.read_csv(\"../data/poly_regression_small.csv\")\n",
    "X_small, Y_small = data_small[\"x\"], data_small[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(loss, grad_loss, D, theta0, alpha, batch_size, n_epochs):\n",
    "    X, y = D  # Unpack the data\n",
    "    d = theta0.shape[0] # While theta0 has shape (d, )\n",
    "    idx = np.arange(0, N) # This is required for the shuffling\n",
    "\n",
    "    # Initialization of history vectors\n",
    "    theta_history = np.zeros((n_epochs, d))  # Save parameters at each epoch\n",
    "    loss_history = np.zeros((n_epochs, ))  # Save loss values at each epoch\n",
    "    grad_norm_history = np.zeros((n_epochs, ))  # Save gradient norms at each epoch\n",
    "    \n",
    "    # Initialize weights\n",
    "    theta = theta0\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        # Initialize a vector that saves the gradient of the loss at each iteration\n",
    "        grad_loss_vec = []\n",
    "\n",
    "        for batch_start in range(0, N, batch_size):\n",
    "            batch_end = min(batch_start + batch_size, N)\n",
    "            X_batch = X[batch_start:batch_end]\n",
    "            y_batch = y[batch_start:batch_end]\n",
    "            \n",
    "            # Compute the gradient of the loss\n",
    "            gradient = grad_loss(theta, X_batch, y_batch)\n",
    "            grad_loss_vec.append(np.linalg.norm(gradient, 2))\n",
    "\n",
    "            # Update weights\n",
    "            theta = theta - alpha * gradient\n",
    "\n",
    "        # Save the updated values\n",
    "        theta_history[epoch] = theta\n",
    "        loss_history[epoch] = loss(theta, X, y)\n",
    "        grad_norm_history[epoch] = np.mean(grad_loss_vec)\n",
    "    \n",
    "    return theta_history, loss_history, grad_norm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(loss, grad_loss, D, theta0, alpha, n_epochs):\n",
    "    X, y = D  # Unpack the data\n",
    "    N = X.shape[0] # We assume both X and Y has shape (N, )\n",
    "    d = theta0.shape[0] # While theta0 has shape (d, )\n",
    "    idx = np.arange(0, N) # This is required for the shuffling\n",
    "\n",
    "    # Initialization of history vectors\n",
    "    theta_history = np.zeros((n_epochs, d))  # Save parameters at each epoch\n",
    "    loss_history = np.zeros((n_epochs, ))  # Save loss values at each epoch\n",
    "    grad_norm_history = np.zeros((n_epochs, ))  # Save gradient norms at each epoch\n",
    "    \n",
    "    # Initialize weights\n",
    "    theta = theta0\n",
    "    for epoch in range(n_epochs):\n",
    "        # Compute the gradient of the loss\n",
    "        gradient = grad_loss(theta, X, y)\n",
    "        grad_norm_history[epoch] = np.linalg.norm(gradient, 2)\n",
    "\n",
    "        # Update weights\n",
    "        theta = theta - alpha * gradient\n",
    "\n",
    "        # Save the updated values\n",
    "        theta_history[epoch] = theta\n",
    "        loss_history[epoch] = loss(theta, X, y)\n",
    "    \n",
    "    return theta_history, loss_history, grad_norm_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalEquations(K, D):\n",
    "    x, y = D\n",
    "    N = x.shape[0]\n",
    "    PhiX = vandermonde(x, K)\n",
    "    # solve X XT theta = X Y\n",
    "    L = np.linalg.cholesky(PhiX @ PhiX.T)\n",
    "    # solve L z = X Y\n",
    "    z = np.linalg.solve(L, PhiX @ y)\n",
    "    # solve LT theta = z\n",
    "    theta = np.linalg.solve(L.T, z)\n",
    "    return theta\n",
    "\n",
    "def vandermonde(x, K):\n",
    "    v = np.ones((K, N))\n",
    "    print(N)\n",
    "    print(x.shape)\n",
    "    print(v.shape)\n",
    "    for i in range(1, K):\n",
    "        v[i] = x ** i\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Dot product shape mismatch, (5,) vs (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m         gradient[i] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39mN) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm((x\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m ((x \u001b[38;5;241m@\u001b[39m theta[i]) \u001b[38;5;241m-\u001b[39m y)), \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m N\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gradient\n\u001b[1;32m---> 29\u001b[0m theta_SGD \u001b[38;5;241m=\u001b[39m \u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_large\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m theta_GD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m     31\u001b[0m theta_NE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "Cell \u001b[1;32mIn[7], line 28\u001b[0m, in \u001b[0;36mSGD\u001b[1;34m(loss, grad_loss, D, theta0, alpha, batch_size, n_epochs)\u001b[0m\n\u001b[0;32m     25\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y[batch_start:batch_end]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Compute the gradient of the loss\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m gradient \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m grad_loss_vec\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(gradient, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Update weights\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 25\u001b[0m, in \u001b[0;36mgrad_loss\u001b[1;34m(theta, x, y)\u001b[0m\n\u001b[0;32m     23\u001b[0m gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(theta)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(theta)):\n\u001b[1;32m---> 25\u001b[0m     gradient[i] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m/\u001b[39mN) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm((x\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m ((\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m-\u001b[39m y)), \u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m/\u001b[39m N\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gradient\n",
      "File \u001b[1;32mc:\\Users\\matti\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:3252\u001b[0m, in \u001b[0;36mSeries.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   3248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m   3249\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3250\u001b[0m \u001b[38;5;124;03m    Matrix multiplication using binary `@` operator.\u001b[39;00m\n\u001b[0;32m   3251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matti\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:3233\u001b[0m, in \u001b[0;36mSeries.dot\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   3231\u001b[0m     rvals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(other)\n\u001b[0;32m   3232\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lvals\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m rvals\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m-> 3233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m   3234\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDot product shape mismatch, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlvals\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrvals\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3235\u001b[0m         )\n\u001b[0;32m   3237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, ABCDataFrame):\n\u001b[0;32m   3238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(\n\u001b[0;32m   3239\u001b[0m         np\u001b[38;5;241m.\u001b[39mdot(lvals, rvals), index\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mcolumns, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   3240\u001b[0m     )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mException\u001b[0m: Dot product shape mismatch, (5,) vs (1,)"
     ]
    }
   ],
   "source": [
    "K = 5\n",
    "N = X_small.shape[0]\n",
    "batch_size = 5\n",
    "n_epochs = 100\n",
    "theta0 = np.zeros((K,1))\n",
    "alpha = 0.1\n",
    "\n",
    "# MLE\n",
    "def loss(theta, x, y):\n",
    "    # return (1/2) * np.linalg.norm((vandermonde(x, K) @ theta - y)**2, 2)\n",
    "    return np.square(np.linalg.norm(x.T @ ((x @ theta) - y), 2)) / N\n",
    "\n",
    "def grad_loss(theta, x, y):\n",
    "    # N = len(y)\n",
    "    # y_pred = f(x, y, theta)\n",
    "    # error = y_pred - y\n",
    "    \n",
    "    # gradient = np.zeros_like(theta)\n",
    "    # for i in range(len(theta)):\n",
    "    #     gradient[i] = 2 * np.sum(error * (x ** i)) / N\n",
    "    # return gradient\n",
    "    \n",
    "    gradient = np.zeros_like(theta)\n",
    "    for i in range(len(theta)):\n",
    "        gradient[i] = (2/N) * np.linalg.norm((x.T @ ((x @ theta[i]) - y)), 2) / N\n",
    "    \n",
    "    return gradient\n",
    "\n",
    "theta_SGD = SGD(loss, grad_loss, (X_small, Y_large), theta0, alpha, batch_size, n_epochs)\n",
    "theta_GD = ...\n",
    "theta_NE = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the relative error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m err_SGD \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_test_large)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([loss(theta_SGD, x, y) \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(X_test_large, Y_test_large))])\n\u001b[0;32m      2\u001b[0m err_GD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m      3\u001b[0m err_NE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m err_SGD \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(Y_test_large)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m([\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_SGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(X_test_large, Y_test_large))])\n\u001b[0;32m      2\u001b[0m err_GD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m      3\u001b[0m err_NE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m, in \u001b[0;36mloss\u001b[1;34m(theta, x, y)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(theta, x, y):\n\u001b[1;32m---> 12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(y_pred \u001b[38;5;241m-\u001b[39m y)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(error)\n",
      "Cell \u001b[1;32mIn[19], line 9\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x, y, theta)\u001b[0m\n\u001b[0;32m      6\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# MLE\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, y, theta: (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm((\u001b[43mvandermonde1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m theta \u001b[38;5;241m-\u001b[39m y)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(theta, x, y):\n\u001b[0;32m     12\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m f(x, y, theta)\n",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m, in \u001b[0;36mvandermonde1\u001b[1;34m(x, K)\u001b[0m\n\u001b[0;32m     23\u001b[0m v \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((K))\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, K):\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m i\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m v\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "err_SGD = (1/ len(Y_test_large)) * sum([loss(theta_SGD, x, y) for x, y in list(zip(X_test_large, Y_test_large))])\n",
    "err_GD = ...\n",
    "err_NE = ...\n",
    "\n",
    "# err_M_s = (1/ len(Y_test_large)) * sum([loss(theta, x, y) for x, y in list(zip(X_test_large, Y_test_large))])\n",
    "print(err_SGD)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
